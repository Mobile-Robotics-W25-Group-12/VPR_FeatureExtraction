{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Pre-Extraction for SLAM\n",
    "\n",
    "This notebook extracts features from dataset and saves them locally in .npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "from feature_extraction.feature_extractor_holistic import AlexNetConv3Extractor, HDCDELF, SAD\n",
    "from feature_extraction.feature_extractor_patchnetvlad import PatchNetVLADFeatureExtractor\n",
    "from feature_extraction.feature_extractor_cosplace import CosPlaceFeatureExtractor\n",
    "from feature_extraction.feature_extractor_eigenplaces import EigenPlacesFeatureExtractor\n",
    "from feature_extraction.feature_extractor_boq import BoQFeatureExtractor\n",
    "from patchnetvlad.tools import PATCHNETVLAD_ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset Input Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Widget for Dataset Path ---\n",
    "dataset_path_widget = widgets.Text(\n",
    "    value='data',\n",
    "    placeholder='Enter path to image directory',\n",
    "    description='Dataset Path:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "display(dataset_path_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path from widget\n",
    "dataset_path = dataset_path_widget.value\n",
    "\n",
    "# Find images\n",
    "image_extensions = ['*.jpg', '*.png', '*.jpeg', '*.bmp', '*.tiff']\n",
    "image_paths = []\n",
    "if os.path.isdir(dataset_path):\n",
    "    print(f\"Searching for images in: {dataset_path}\")\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "\n",
    "        pattern = os.path.join(dataset_path, '**', ext)\n",
    "        image_paths.extend(glob.glob(pattern, recursive=True))\n",
    "\n",
    "    image_paths = sorted(image_paths)\n",
    "\n",
    "    if not image_paths:\n",
    "         print(f\"Warning: No images found with extensions {image_extensions} in {dataset_path}\")\n",
    "    else:\n",
    "         print(f\"Found {len(image_paths)} images.\")\n",
    "else:\n",
    "    print(f\"Error: Dataset path not found or is not a directory: {dataset_path}\")\n",
    "    image_paths = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Extraction Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Selecting a descriptor ---\n",
    "descriptor_widget = widgets.Dropdown(\n",
    "    options=['HDC-DELF', 'AlexNet', 'NetVLAD', 'PatchNetVLAD', 'CosPlace', 'EigenPlaces', 'SAD', 'BoQ-ResNet50', 'BoQ-DinoV2'],\n",
    "    value='BoQ-DinoV2',\n",
    "    description='Descriptor:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(descriptor_widget)\n",
    "\n",
    "# --- Batch Size ---\n",
    "batch_size_widget = widgets.IntSlider(\n",
    "    value=8,\n",
    "    min=1,\n",
    "    max=64,\n",
    "    step=1,\n",
    "    description='Batch Size:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "display(batch_size_widget)\n",
    "\n",
    "# --- Output Directory ---\n",
    "output_dir_widget = widgets.Text(\n",
    "    value='feature_extraction_output',\n",
    "    placeholder='Enter base directory for output',\n",
    "    description='Output Dir:',\n",
    "    disabled=False,\n",
    "     style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "display(output_dir_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get selections from widgets ---\n",
    "selected_model = descriptor_widget.value\n",
    "base_output_dir = output_dir_widget.value\n",
    "batch_size = batch_size_widget.value\n",
    "\n",
    "# --- Setup Output Dirs ---\n",
    "csv_dir = os.path.join(base_output_dir, 'csv')\n",
    "npy_dir = os.path.join(base_output_dir, 'npy', selected_model)\n",
    "combined_npy_dir = os.path.join(base_output_dir, 'npy', selected_model + '-stacked')\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "os.makedirs(npy_dir, exist_ok=True)\n",
    "os.makedirs(combined_npy_dir, exist_ok=True)\n",
    "csv_output_path = os.path.join(csv_dir, f\"metadata_{selected_model}.csv\")\n",
    "print(f\"CSV will be saved to: {csv_output_path}\")\n",
    "print(f\".npy files will be saved to: {npy_dir}\")\n",
    "print(f\"Combined .npy feature vector will be saved to: {combined_npy_dir}\")\n",
    "\n",
    "\n",
    "# --- Model Configuration & Initialization ---\n",
    "feature_extractor = None\n",
    "print(f\"Initializing feature extractor: {selected_model}...\")\n",
    "if selected_model == 'AlexNet':\n",
    "    feature_extractor = AlexNetConv3Extractor()\n",
    "elif selected_model == 'HDC-DELF':\n",
    "    feature_extractor = HDCDELF()\n",
    "elif selected_model == 'SAD':\n",
    "    feature_extractor = SAD()\n",
    "elif selected_model in ['NetVLAD', 'PatchNetVLAD']:\n",
    "    if selected_model == 'NetVLAD':\n",
    "        config_file = os.path.join(PATCHNETVLAD_ROOT_DIR, 'configs/netvlad_extract.ini')\n",
    "    else:\n",
    "        config_file = os.path.join(PATCHNETVLAD_ROOT_DIR, 'configs/speed.ini')\n",
    "\n",
    "    if not os.path.isfile(config_file):\n",
    "        print(f\"Error: Config file not found for {selected_model} at {config_file}\")\n",
    "        raise FileNotFoundError(f\"PatchNetVLAD config missing: {config_file}\")\n",
    "\n",
    "    print(f\"  Using config: {config_file}\")\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    feature_extractor = PatchNetVLADFeatureExtractor(config)\n",
    "elif selected_model == 'CosPlace':\n",
    "    feature_extractor = CosPlaceFeatureExtractor()\n",
    "elif selected_model == 'EigenPlaces':\n",
    "    feature_extractor = EigenPlacesFeatureExtractor()\n",
    "elif selected_model == 'BoQ-ResNet50':\n",
    "    feature_extractor = BoQFeatureExtractor(backbone_name=\"resnet50\")\n",
    "elif selected_model == 'BoQ-DinoV2':\n",
    "    feature_extractor = BoQFeatureExtractor(backbone_name=\"dinov2\")\n",
    "else:\n",
    "    print(f\"Error: Model '{selected_model}' is not recognized for initialization.\")\n",
    "    raise ValueError(f\"Unsupported model: {selected_model}\")\n",
    "\n",
    "print(\"Feature extractor initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Extraction Loop ---\n",
    "metadata_list = []\n",
    "ordered_features = []\n",
    "total_start_time = time.time()\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(image_paths)} images...\")\n",
    "\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:min(i + batch_size, len(image_paths))]\n",
    "    batch_images_data = []\n",
    "    batch_valid_indices = []\n",
    "\n",
    "    # Load images for the current batch\n",
    "    for idx, img_path in enumerate(batch_paths):\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            batch_images_data.append(np.array(img))\n",
    "            batch_valid_indices.append(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load image {os.path.basename(img_path)}. Skipping. Error: {e}\")\n",
    "            failed_count += 1\n",
    "\n",
    "    if not batch_images_data:\n",
    "        if batch_paths:\n",
    "             print(f\"Warning: No images could be loaded in batch starting with {os.path.basename(batch_paths[0])}. Skipping batch.\")\n",
    "        continue\n",
    "\n",
    "    # Process the valid images in the batch\n",
    "    try:\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        feature_vectors_batch = feature_extractor.compute_features(batch_images_data)\n",
    "        batch_end_time = time.time()\n",
    "        batch_processing_time = batch_end_time - batch_start_time\n",
    "        time_per_image = batch_processing_time / len(batch_images_data) if batch_images_data else 0\n",
    "\n",
    "        # Check if output length matches input length\n",
    "        if len(feature_vectors_batch) != len(batch_images_data):\n",
    "             print(f\"Error: Mismatch between input batch size ({len(batch_images_data)}) and output features ({len(feature_vectors_batch)}) for batch starting with {os.path.basename(batch_paths[0])}. Skipping results for this batch.\")\n",
    "             failed_count += len(batch_images_data)\n",
    "             continue\n",
    "\n",
    "        # Save features and record metadata for successfully processed images\n",
    "        for batch_idx, feature_vector in enumerate(feature_vectors_batch):\n",
    "\n",
    "            original_index_in_batch = batch_valid_indices[batch_idx]\n",
    "            img_filepath = batch_paths[original_index_in_batch]\n",
    "            frame_id = os.path.basename(img_filepath)\n",
    "            feature_dim = feature_vector.shape\n",
    "            ordered_features.append(feature_vector)\n",
    "\n",
    "            # Define .npy output path\n",
    "            npy_filename = f\"{os.path.splitext(frame_id)[0]}.npy\"\n",
    "            feature_path_npy = os.path.join(npy_dir, npy_filename)\n",
    "\n",
    "            # Save .npy file\n",
    "            np.save(feature_path_npy, feature_vector)\n",
    "\n",
    "            # Store Metadata\n",
    "            metadata_list.append({\n",
    "                'frame_id': frame_id,\n",
    "                'model_name': selected_model,\n",
    "                'processing_time_sec': time_per_image,\n",
    "                'feature_dim': str(feature_dim),\n",
    "                'feature_path_npy': feature_path_npy,\n",
    "                'original_path': img_filepath\n",
    "            })\n",
    "            processed_count += 1\n",
    "\n",
    "        print(f\"  Processed batch {i//batch_size + 1}/{ (len(image_paths) + batch_size - 1)//batch_size} ({processed_count}/{len(image_paths)} images done)\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch starting with {os.path.basename(batch_paths[0])}: {e}\")\n",
    "        failed_count += len(batch_images_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Summary & Save Metadata ---\n",
    "total_end_time = time.time()\n",
    "print(\"\\n--- Extraction Summary ---\")\n",
    "print(f\"Successfully processed: {processed_count} images\")\n",
    "if failed_count > 0:\n",
    "    print(f\"Failed/Skipped:       {failed_count} images\")\n",
    "print(f\"Total processing time: {total_end_time - total_start_time:.2f} seconds.\")\n",
    "\n",
    "if metadata_list:\n",
    "    metadata_df = pd.DataFrame(metadata_list)\n",
    "    cols_order = ['frame_id', 'model_name', 'processing_time_sec', 'feature_dim', 'feature_path_npy', 'original_path']\n",
    "    metadata_df = metadata_df[[col for col in cols_order if col in metadata_df.columns]]\n",
    "\n",
    "\n",
    "    # Save to CSV\n",
    "    metadata_df.to_csv(csv_output_path, index=False)\n",
    "    print(f\"\\nMetadata saved to: {csv_output_path}\")\n",
    "    print(f\".npy features saved in: {npy_dir}\")\n",
    "\n",
    "    # Display head in notebook\n",
    "    print(\"\\nMetadata Preview (first 5 rows):\")\n",
    "    display(metadata_df.head())\n",
    "\n",
    "    # Stack features from list and save them\n",
    "    ordered_features = np.vstack(ordered_features)\n",
    "    npy_filename = \"stacked_features.npy\"\n",
    "    combined_npy_path = os.path.join(combined_npy_dir, npy_filename) \n",
    "    np.save(combined_npy_path, ordered_features)\n",
    "    print(f\"Stacked features of {ordered_features.shape} shape saved to: {combined_npy_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo metadata was generated (or all images failed).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROB530ProjectGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
