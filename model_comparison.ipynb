{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080e457e",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "This notebook compares different Visual Place Recognition models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735ac3c",
   "metadata": {},
   "source": [
    "## Select Models and Datasets of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load_dataset import GardensPointDataset, StLuciaDataset, SFUDataset\n",
    "from patchnetvlad.tools import PATCHNETVLAD_ROOT_DIR\n",
    "\n",
    "\n",
    "# List of models to evaluate\n",
    "models = ['NetVLAD','PatchNetVLAD','BoQ-DinoV2','CosPlace']\n",
    "\n",
    "# Create dataset objects\n",
    "datasets = [\n",
    "    ('GardensPoint', GardensPointDataset()),\n",
    "    ('StLucia', StLuciaDataset()),\n",
    "    ('SFU', SFUDataset())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cc1c5",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4574f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Import functions from the VPR toolkit\n",
    "from evaluation.metrics import recallAtK\n",
    "\n",
    "# Results storage paths\n",
    "data_path = os.path.join('data/vpr_comparison/')\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "results_path = os.path.join(data_path, 'vpr_comparison_results.json')\n",
    "timing_path = os.path.join(data_path, 'vpr_timing_results.json')\n",
    "\n",
    "# Image paths\n",
    "image_data_paths = 'images/vpr_comparison/'\n",
    "os.makedirs(image_data_paths, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7af54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_one_percent_recall(S, GT):\n",
    "    \"\"\"\n",
    "    Compute the 1% recall metric for a given similarity matrix and ground truth.\n",
    "    \"\"\"\n",
    "    # Calculate how many images constitutes 1% of the database\n",
    "    db_size = S.shape[0]\n",
    "    k = max(1, int(0.01 * db_size))\n",
    "    \n",
    "    # Use the existing recallAtK function\n",
    "    recall_value = recallAtK(S, GT, K=k)\n",
    "    \n",
    "    return recall_value\n",
    "\n",
    "def save_results(results_dict, timing_dict):\n",
    "    \"\"\"Save results and timing to disk\"\"\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results_dict, f)\n",
    "    with open(timing_path, 'w') as f:\n",
    "        json.dump(timing_dict, f)\n",
    "    print(f\"Results and timing data saved\")\n",
    "\n",
    "def load_results():\n",
    "    \"\"\"Load results from disk if available\"\"\"\n",
    "    results = defaultdict(dict)\n",
    "    timing = defaultdict(dict)\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "    \n",
    "    if os.path.exists(timing_path):\n",
    "        with open(timing_path, 'r') as f:\n",
    "            timing = json.load(f)\n",
    "            \n",
    "    return results, timing\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory between runs\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09c2d5",
   "metadata": {},
   "source": [
    "## Evaluate the Models Against Datasets of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad666813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, dataset, batch_size=4, use_cpu=False):\n",
    "    \"\"\"\n",
    "    Evaluate a VPR model on a specific dataset with memory management options.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to evaluate\n",
    "        dataset: Dataset object\n",
    "        batch_size: Batch size for processing (smaller = less memory)\n",
    "        use_cpu: Force CPU usage instead of GPU\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (recall value, timing dictionary)\n",
    "    \"\"\"\n",
    "    print(f\"===== Evaluating {model_name} on {dataset.__class__.__name__} =====\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cpu') if use_cpu else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize timing dictionary\n",
    "    timing = {\n",
    "        'feature_extraction_db': 0,\n",
    "        'feature_extraction_q': 0,\n",
    "        'matching': 0,\n",
    "        'total': 0\n",
    "    }\n",
    "    \n",
    "    start_time_total = time.time()\n",
    "    \n",
    "    # Load dataset\n",
    "    imgs_db, imgs_q, GThard, GTsoft = dataset.load()\n",
    "    \n",
    "    # Initialize feature extractor based on model name\n",
    "    feature_extractor = None\n",
    "    try:\n",
    "        if model_name == 'AlexNet':\n",
    "            from feature_extraction.feature_extractor_holistic import AlexNetConv3Extractor\n",
    "            feature_extractor = AlexNetConv3Extractor()\n",
    "            \n",
    "        elif model_name == 'NetVLAD':\n",
    "            from feature_extraction.feature_extractor_patchnetvlad import PatchNetVLADFeatureExtractor\n",
    "            from patchnetvlad.tools import PATCHNETVLAD_ROOT_DIR\n",
    "            configfile = os.path.join(PATCHNETVLAD_ROOT_DIR, 'configs/netvlad_extract.ini')\n",
    "            assert os.path.isfile(configfile)\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read(configfile)\n",
    "            feature_extractor = PatchNetVLADFeatureExtractor(config)\n",
    "\n",
    "        elif model_name == 'PatchNetVLAD':\n",
    "            from feature_extraction.feature_extractor_patchnetvlad import PatchNetVLADFeatureExtractor\n",
    "            from patchnetvlad.tools import PATCHNETVLAD_ROOT_DIR\n",
    "            configfile = os.path.join(PATCHNETVLAD_ROOT_DIR, 'configs/speed.ini')\n",
    "            assert os.path.isfile(configfile)\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read(configfile)\n",
    "            feature_extractor = PatchNetVLADFeatureExtractor(config)\n",
    "\n",
    "        elif model_name == 'HDC-DELF':\n",
    "            from feature_extraction.feature_extractor_holistic import HDCDELF\n",
    "            feature_extractor = HDCDELF()\n",
    "        elif model_name == 'SAD':\n",
    "            from feature_extraction.feature_extractor_holistic import SAD\n",
    "            feature_extractor = SAD()\n",
    "        elif model_name == 'CosPlace':\n",
    "            from feature_extraction.feature_extractor_cosplace import CosPlaceFeatureExtractor\n",
    "            feature_extractor = CosPlaceFeatureExtractor()\n",
    "        elif model_name == 'EigenPlaces':\n",
    "            from feature_extraction.feature_extractor_eigenplaces import EigenPlacesFeatureExtractor\n",
    "            feature_extractor = EigenPlacesFeatureExtractor()\n",
    "            \n",
    "        elif model_name == 'BoQ-DinoV2':\n",
    "            from feature_extraction.feature_extractor_boq import BoQFeatureExtractor\n",
    "            # Use a smaller batch size for BoQ-DinoV2 to reduce memory usage\n",
    "            feature_extractor = BoQFeatureExtractor(backbone_name=\"dinov2\")\n",
    "            \n",
    "            # Override the compute_features method to process in smaller batches\n",
    "            original_compute_features = feature_extractor.compute_features\n",
    "            \n",
    "            def batched_compute_features(imgs):\n",
    "                print(f\"Processing in batches of {batch_size}...\")\n",
    "                results = []\n",
    "                for i in range(0, len(imgs), batch_size):\n",
    "                    batch = imgs[i:i+batch_size]\n",
    "                    print(f\"Processing batch {i//batch_size + 1}/{len(imgs)//batch_size + 1}\")\n",
    "                    batch_results = original_compute_features(batch)\n",
    "                    results.append(batch_results)\n",
    "                    # Clear memory after each batch\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                return np.vstack(results)\n",
    "            \n",
    "            feature_extractor.compute_features = batched_compute_features\n",
    "\n",
    "        elif model_name == 'SuperPoint':\n",
    "            from feature_extraction.feature_extractor_superpoint import SuperPoint\n",
    "            feature_extractor = SuperPoint()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "        # Move model to specified device if it has parameters\n",
    "        if hasattr(feature_extractor, 'model'):\n",
    "            feature_extractor.model = feature_extractor.model.to(device)\n",
    "        \n",
    "        # Compute descriptors for reference set\n",
    "        print('===== Computing reference set descriptors =====')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Special handling for PatchNetVLAD and SAD\n",
    "        if model_name == 'PatchNetVLAD':\n",
    "            db_D_holistic, db_D_patches = feature_extractor.compute_features(imgs_db)\n",
    "            timing['feature_extraction_db'] = time.time() - start_time\n",
    "            \n",
    "            print('===== Computing query set descriptors =====')\n",
    "            start_time = time.time()\n",
    "            q_D_holistic, q_D_patches = feature_extractor.compute_features(imgs_q)\n",
    "            timing['feature_extraction_q'] = time.time() - start_time\n",
    "            \n",
    "            # Compute similarity matrix with patch matching\n",
    "            print('===== Computing similarities with patch matching =====')\n",
    "            start_time = time.time()\n",
    "            S = feature_extractor.local_matcher_from_numpy_single_scale(q_D_patches, db_D_patches)\n",
    "            timing['matching'] = time.time() - start_time\n",
    "\n",
    "        elif model_name == 'SAD':\n",
    "            db_D_holistic = feature_extractor.compute_features(imgs_db)\n",
    "            timing['feature_extraction_db'] = time.time() - start_time\n",
    "            \n",
    "            print('===== Computing query set descriptors =====')\n",
    "            start_time = time.time()\n",
    "            q_D_holistic = feature_extractor.compute_features(imgs_q)\n",
    "            timing['feature_extraction_q'] = time.time() - start_time\n",
    "            \n",
    "            # Compute similarity matrix with SAD\n",
    "            print('===== Computing similarities with sum of absolute differences =====')\n",
    "            start_time = time.time()\n",
    "            S = np.empty([len(imgs_db), len(imgs_q)], \"float32\")\n",
    "            for i in range(S.shape[0]):\n",
    "                for j in range(S.shape[1]):\n",
    "                    diff = db_D_holistic[i] - q_D_holistic[j]\n",
    "                    dim = len(db_D_holistic[0]) - np.sum(np.isnan(diff))\n",
    "                    diff[np.isnan(diff)] = 0\n",
    "                    S[i, j] = -np.sum(np.abs(diff)) / dim\n",
    "            timing['matching'] = time.time() - start_time\n",
    "        elif model_name == 'SuperPoint':\n",
    "            \n",
    "            # Compute database features\n",
    "            db_features = feature_extractor.compute_features(imgs_db)\n",
    "            timing['feature_extraction_db'] = time.time() - start_time\n",
    "            \n",
    "            print('===== Computing query set descriptors =====')\n",
    "            start_time = time.time()\n",
    "            q_features = feature_extractor.compute_features(imgs_q)\n",
    "            timing['feature_extraction_q'] = time.time() - start_time\n",
    "            \n",
    "            # Compute similarity matrix with SuperPoint matcher\n",
    "            print('===== Computing similarities with SuperPoint matcher =====')\n",
    "            start_time = time.time()\n",
    "            S = feature_extractor.local_matcher_from_numpy_single_scale(q_features, db_features)\n",
    "            timing['matching'] = time.time() - start_time\n",
    "        else:\n",
    "            # Standard processing for other models\n",
    "            db_D_holistic = feature_extractor.compute_features(imgs_db)\n",
    "            timing['feature_extraction_db'] = time.time() - start_time\n",
    "            \n",
    "            print('===== Computing query set descriptors =====')\n",
    "            start_time = time.time()\n",
    "            q_D_holistic = feature_extractor.compute_features(imgs_q)\n",
    "            timing['feature_extraction_q'] = time.time() - start_time\n",
    "            \n",
    "            # Compute similarity matrix\n",
    "            print('===== Computing similarities =====')\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Normalize descriptors and compute S-matrix\n",
    "            db_D_holistic = db_D_holistic / np.linalg.norm(db_D_holistic, axis=1, keepdims=True)\n",
    "            q_D_holistic = q_D_holistic / np.linalg.norm(q_D_holistic, axis=1, keepdims=True)\n",
    "            S = np.matmul(db_D_holistic, q_D_holistic.transpose())\n",
    "            timing['matching'] = time.time() - start_time\n",
    "        \n",
    "        # Compute 1% recall using the GTsoft for evaluation\n",
    "        recall_value = compute_one_percent_recall(S, GTsoft)\n",
    "        print(f\"1% Recall: {recall_value:.3f}\")\n",
    "        \n",
    "        # Calculate total time\n",
    "        timing['total'] = time.time() - start_time_total\n",
    "        print(f\"Total time: {timing['total']:.2f} seconds\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        return recall_value, timing\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(f\"CUDA out of memory error encountered. Try with a smaller batch size or use CPU.\")\n",
    "            if not use_cpu:\n",
    "                print(\"Attempting to run on CPU instead...\")\n",
    "                # Clear GPU memory\n",
    "                clear_gpu_memory()\n",
    "                # Try again on CPU\n",
    "                return evaluate_model(model_name, dataset, batch_size, use_cpu=True)\n",
    "            else:\n",
    "                print(\"Failed even on CPU. This model might be too large for your system.\")\n",
    "        else:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Helper function to clear GPU memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any previously saved results\n",
    "results, timing = load_results()\n",
    "\n",
    "# Initialize results and timing dictionaries if they don't exist\n",
    "if results is None:\n",
    "    results = {}\n",
    "\n",
    "# Run evaluations one by one with result caching\n",
    "for dataset_name, dataset_obj in datasets:\n",
    "    print(f\"\\n===== Processing dataset: {dataset_name} =====\")\n",
    "    \n",
    "    # Initialize dictionary for this dataset if it doesn't exist\n",
    "    if dataset_name not in results:\n",
    "        results[dataset_name] = {}\n",
    "    if dataset_name not in timing:\n",
    "        timing[dataset_name] = {}\n",
    "    \n",
    "    # Skip if we've already processed all models for this dataset\n",
    "    if all(model in results[dataset_name] for model in models):\n",
    "        print(f\"Already evaluated all models for {dataset_name}, skipping.\")\n",
    "        continue\n",
    "        \n",
    "    for model_name in models:\n",
    "        # Skip if we've already evaluated this model on this dataset\n",
    "        if model_name in results[dataset_name]:\n",
    "            print(f\"Already evaluated {model_name} on {dataset_name}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nEvaluating {model_name} on {dataset_name}...\")\n",
    "        \n",
    "        # For BoQ-DinoV2, use smaller batch size\n",
    "        batch_size = 1 if model_name == 'BoQ-DinoV2' else 4\n",
    "        \n",
    "        # Evaluate model\n",
    "        recall, model_timing = evaluate_model(model_name, dataset_obj, batch_size=batch_size)\n",
    "        \n",
    "        # Store result\n",
    "        if recall is not None:\n",
    "            results[dataset_name][model_name] = recall\n",
    "            timing[dataset_name][model_name] = model_timing\n",
    "            \n",
    "            # Save after each successful evaluation\n",
    "            save_results(results, timing)\n",
    "            \n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n===== Results Summary =====\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d177a0",
   "metadata": {},
   "source": [
    "## Generate Comparison Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb15711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ea0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_charts(results_df, timing_dict):\n",
    "    \"\"\"\n",
    "    Create high-resolution charts for poster presentation showing recall performance,\n",
    "    timing metrics, and efficiency using University of Michigan colors.\n",
    "    Models are sorted by their publication year.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert recall values from decimal to percentage\n",
    "    results_df = results_df * 100  # Convert to percentage\n",
    "    \n",
    "    # Set high DPI for high resolution\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    \n",
    "    # University of Michigan official colors\n",
    "    UMICH_BLUE = '#00274C'\n",
    "    UMICH_MAIZE = '#FFCB05'\n",
    "    \n",
    "    # Create a color palette based on UMich colors\n",
    "    color_palette = [\n",
    "        UMICH_BLUE,     \n",
    "        UMICH_MAIZE,   \n",
    "        '#1D3B60',      \n",
    "        '#FFD633',       \n",
    "        '#3A5273',       \n",
    "        '#FFE066',  \n",
    "        '#576F93',     \n",
    "        '#FFEB99',      \n",
    "    ]\n",
    "\n",
    "    # Fonts\n",
    "    SMALL_SIZE = 14\n",
    "    MEDIUM_SIZE = 16\n",
    "    BIGGER_SIZE = 20\n",
    "    TITLE_SIZE = 24\n",
    "    \n",
    "    # Configure font sizes\n",
    "    plt.rc('font', size=SMALL_SIZE, weight='bold', family='sans-serif')\n",
    "    plt.rc('axes', titlesize=BIGGER_SIZE, labelsize=MEDIUM_SIZE)\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "    plt.rc('figure', titlesize=TITLE_SIZE)\n",
    "    \n",
    "    # Apply UMich style to matplotlib\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "    plt.rcParams['axes.edgecolor'] = UMICH_BLUE\n",
    "    plt.rcParams['axes.labelcolor'] = UMICH_BLUE\n",
    "    plt.rcParams['axes.labelweight'] = 'bold'\n",
    "    plt.rcParams['axes.titleweight'] = 'bold'\n",
    "    plt.rcParams['axes.spines.right'] = False\n",
    "    plt.rcParams['axes.spines.top'] = False\n",
    "    plt.rcParams['xtick.color'] = UMICH_BLUE\n",
    "    plt.rcParams['ytick.color'] = UMICH_BLUE\n",
    "    \n",
    "    # Define model years dictionary\n",
    "    model_years = {\n",
    "        'NetVLAD': '2016',\n",
    "        'PatchNetVLAD': '2021',\n",
    "        'BoQ-DinoV2': '2024',\n",
    "        'BoQ-ResNet50': '2024',\n",
    "        'AlexNet': '2012',\n",
    "        'EigenPlaces': '2023',\n",
    "        'CosPlace': '2022',\n",
    "        'HDC-DELF': '2019',\n",
    "        'SAD': '2021',\n",
    "        'SuperPoint': '2017',\n",
    "    }\n",
    "    \n",
    "    # Create a dictionary of actual models in our results with their years\n",
    "    actual_model_years = {}\n",
    "    for model in results_df.index:\n",
    "        if model in model_years:\n",
    "            actual_model_years[model] = model_years[model]\n",
    "        else:\n",
    "            actual_model_years[model] = '9999'  # Unknown years go to the end\n",
    "    \n",
    "    # Sort models by year\n",
    "    sorted_models = sorted(actual_model_years.keys(), key=lambda x: (actual_model_years[x], x))\n",
    "    \n",
    "    # Reindex results to match the sorted order\n",
    "    sorted_results_df = results_df.reindex(sorted_models)\n",
    "    \n",
    "    # Create timing DataFrame from sorted models\n",
    "    timing_data = {}\n",
    "    for model in sorted_models:\n",
    "        timing_data[model] = {}\n",
    "        for dataset in sorted_results_df.columns:\n",
    "            if dataset in timing_dict and model in timing_dict[dataset]:\n",
    "                timing_data[model][dataset] = timing_dict[dataset][model]['total']\n",
    "            else:\n",
    "                timing_data[model][dataset] = np.nan  # Use NaN for missing data\n",
    "    \n",
    "    timing_df = pd.DataFrame(timing_data).transpose()\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n",
    "    \n",
    "    # Setup for recall chart (left)\n",
    "    n_datasets = len(sorted_results_df.columns)\n",
    "    n_models = len(sorted_models)\n",
    "    bar_width = 0.7 / n_models \n",
    "    r = np.arange(n_datasets)\n",
    "    \n",
    "    # Plot recall bars\n",
    "    for i, model in enumerate(sorted_models):\n",
    "        if model in model_years:\n",
    "            model_with_year = f\"{model} ({model_years[model]})\"\n",
    "        else:\n",
    "            model_with_year = model\n",
    "        \n",
    "        color_idx = i % len(color_palette)\n",
    "        \n",
    "        ax1.bar(r + i*bar_width - (n_models-1)*bar_width/2, sorted_results_df.loc[model], \n",
    "                width=bar_width, label=model_with_year, alpha=0.9,\n",
    "                color=color_palette[color_idx], edgecolor=UMICH_BLUE, linewidth=1)\n",
    "    \n",
    "    # Set recall chart properties with enhanced styling\n",
    "    ax1.set_xlabel('Dataset', fontweight='bold', fontsize=MEDIUM_SIZE)\n",
    "    ax1.set_ylabel('1% Recall (%)', fontweight='bold', fontsize=MEDIUM_SIZE)\n",
    "    ax1.set_title('Recall Performance Across Datasets', fontsize=BIGGER_SIZE, fontweight='bold')\n",
    "    ax1.set_xticks(r)\n",
    "    ax1.set_xticklabels(sorted_results_df.columns, fontweight='bold', rotation=0)\n",
    "    ax1.set_ylim(0, 105)  # Set y-axis limit for percentage (0-105%)\n",
    "    \n",
    "    # Use UMich style grid\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.5, color=UMICH_BLUE)\n",
    "    \n",
    "    # Add a horizontal line at y=100 to represent perfect recall\n",
    "    ax1.axhline(y=100.0, linestyle='--', alpha=0.7, color=UMICH_MAIZE, linewidth=2)\n",
    "    \n",
    "    # Add percentage labels on top of each bar\n",
    "    for i, model in enumerate(sorted_models):\n",
    "        for j, dataset in enumerate(sorted_results_df.columns):\n",
    "            height = sorted_results_df.loc[model, dataset]\n",
    "            if not np.isnan(height):  # Skip missing values\n",
    "                ax1.text(j + i*bar_width - (n_models-1)*bar_width/2, height + 2, \n",
    "                         f'{height:.1f}%', ha='center', va='bottom', \n",
    "                         fontsize=SMALL_SIZE-4, fontweight='bold', rotation=0,\n",
    "                         color=UMICH_BLUE)\n",
    "    \n",
    "    # Plot timing bars (right)\n",
    "    for i, model in enumerate(sorted_models):\n",
    "        if model in model_years:\n",
    "            model_with_year = f\"{model} ({model_years[model]})\"\n",
    "        else:\n",
    "            model_with_year = model\n",
    "        \n",
    "        # Use color from palette\n",
    "        color_idx = i % len(color_palette)\n",
    "        \n",
    "        ax2.bar(r + i*bar_width - (n_models-1)*bar_width/2, timing_df.loc[model], \n",
    "                width=bar_width, label=model_with_year, alpha=0.9,\n",
    "                color=color_palette[color_idx], edgecolor=UMICH_BLUE, linewidth=1)\n",
    "    \n",
    "    # Set timing chart properties\n",
    "    ax2.set_xlabel('Dataset', fontweight='bold', fontsize=MEDIUM_SIZE)\n",
    "    ax2.set_ylabel('Processing Time (seconds, Lower is Better)', fontweight='bold', fontsize=MEDIUM_SIZE)\n",
    "    ax2.set_title('Processing Time Across Datasets', fontsize=BIGGER_SIZE, fontweight='bold')\n",
    "    ax2.set_xticks(r)\n",
    "    ax2.set_xticklabels(timing_df.columns, fontweight='bold', rotation=0)\n",
    "    \n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.5, color=UMICH_BLUE)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for i, model in enumerate(sorted_models):\n",
    "        for j, dataset in enumerate(timing_df.columns):\n",
    "            height = timing_df.loc[model, dataset] if model in timing_df.index else np.nan\n",
    "            if not np.isnan(height):  \n",
    "                ax2.text(j + i*bar_width - (n_models-1)*bar_width/2, height + 0.02 * ax2.get_ylim()[1], \n",
    "                         f'{height:.1f}s', ha='center', va='bottom', \n",
    "                         fontsize=SMALL_SIZE-4, fontweight='bold', rotation=0,\n",
    "                         color=UMICH_BLUE)\n",
    "    \n",
    "    # Add box around legend\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.01), \n",
    "               ncol=min(5, n_models), frameon=True, fontsize=MEDIUM_SIZE-2,\n",
    "               edgecolor=UMICH_BLUE)\n",
    "    \n",
    "    # Main figure title for the comparison\n",
    "    fig.suptitle('Visual Place Recognition Model Comparison', fontsize=TITLE_SIZE+4, \n",
    "                 fontweight='bold', color=UMICH_BLUE, y=0.98)\n",
    "    \n",
    "    \n",
    "    fig.text(0.99, 0.01, 'University of Michigan', fontsize=SMALL_SIZE, \n",
    "             fontweight='bold', color=UMICH_BLUE, ha='right')\n",
    "    \n",
    "    # Adjust layout and save with high DPI\n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "    vpr_comparison_path = os.path.join(image_data_paths, 'vpr_comparison_with_timing.png')\n",
    "    vpr_comparison_path_pdf = os.path.join(image_data_paths, 'vpr_comparison_with_timing.pdf')\n",
    "    plt.savefig(vpr_comparison_path, dpi=600, bbox_inches='tight')\n",
    "    plt.savefig(vpr_comparison_path_pdf, bbox_inches='tight') \n",
    "    plt.show()\n",
    "    \n",
    "    # Create a normalized time-efficiency chart (recall/second)\n",
    "    efficiency_data = defaultdict(dict)\n",
    "    for dataset in sorted_results_df.columns:\n",
    "        for model in sorted_models:\n",
    "            if dataset in timing_dict and model in timing_dict[dataset]:\n",
    "                recall = sorted_results_df.loc[model, dataset] / 100.0  # Convert back to decimal for calculation\n",
    "                time_taken = timing_dict[dataset][model]['total']\n",
    "                if time_taken > 0:\n",
    "                    efficiency = recall / time_taken  \n",
    "                    efficiency_data[model][dataset] = efficiency\n",
    "    \n",
    "    if not efficiency_data:\n",
    "        print(\"No efficiency data available.\")\n",
    "        return\n",
    "        \n",
    "    efficiency_df = pd.DataFrame(efficiency_data)\n",
    "    efficiency_df = efficiency_df.transpose()\n",
    "    \n",
    "    # Plot efficiency chart with enhanced styling\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    ax = plt.gca()\n",
    "    r = np.arange(len(efficiency_df.columns))\n",
    "    \n",
    "    for i, model in enumerate(sorted_models):\n",
    "        if model in efficiency_df.index and any(not pd.isna(value) for value in efficiency_df.loc[model]):\n",
    "            if model in model_years:\n",
    "                model_with_year = f\"{model} ({model_years[model]})\"\n",
    "            else:\n",
    "                model_with_year = model\n",
    "            \n",
    "            color_idx = i % len(color_palette)\n",
    "            \n",
    "            values = efficiency_df.loc[model].values if model in efficiency_df.index else np.zeros(len(r))\n",
    "            plt.bar(r + i*bar_width - (n_models-1)*bar_width/2, values, width=bar_width, \n",
    "                    label=model_with_year, alpha=0.9, color=color_palette[color_idx], \n",
    "                    edgecolor=UMICH_BLUE, linewidth=1)\n",
    "    \n",
    "            for j, val in enumerate(values):\n",
    "                if not np.isnan(val):\n",
    "                    plt.text(r[j] + i*bar_width - (n_models-1)*bar_width/2, val + 0.01 * ax.get_ylim()[1], \n",
    "                            f'{val:.3f}', ha='center', va='bottom', \n",
    "                            fontsize=SMALL_SIZE-2, fontweight='bold',\n",
    "                            color=UMICH_BLUE)\n",
    "    \n",
    "    plt.xlabel('Dataset', fontweight='bold', fontsize=MEDIUM_SIZE)\n",
    "    plt.ylabel('Efficiency (Recall/Second, Higher is Better)', fontweight='bold', fontsize=MEDIUM_SIZE)\n",
    "    plt.title('VPR Model Efficiency Comparison', fontsize=BIGGER_SIZE, fontweight='bold', color=UMICH_BLUE)\n",
    "    plt.xticks(r, efficiency_df.columns, fontweight='bold')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5, color=UMICH_BLUE)\n",
    "    \n",
    "    # Add box around legend with alpha\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), \n",
    "              ncol=min(5, n_models), frameon=True, fontsize=MEDIUM_SIZE-2,\n",
    "              edgecolor=UMICH_BLUE)\n",
    "    \n",
    "    plt.figtext(0.99, 0.01, 'University of Michigan', fontsize=SMALL_SIZE, \n",
    "                fontweight='bold', color=UMICH_BLUE, ha='right')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "\n",
    "    vpr_efficiency_path = os.path.join(image_data_paths, 'vpr_efficiency.png')\n",
    "    vpr_efficiency_path_pdf = os.path.join(image_data_paths, 'vpr_efficiency.pdf')\n",
    "\n",
    "    plt.savefig(vpr_efficiency_path, dpi=600, bbox_inches='tight')\n",
    "    plt.savefig(vpr_efficiency_path_pdf, bbox_inches='tight') \n",
    "    plt.show()\n",
    "    \n",
    "# Create the visualizations\n",
    "create_comparison_charts(results_df, timing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROB530ProjectGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
